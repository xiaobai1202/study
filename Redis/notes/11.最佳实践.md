# 最佳实践

---

## key 设计

Redis的key虽然可以自定义，但最好遵循下面的几个最佳实践约定：

1. 遵循基本格式： \[业务名称\]:\[数据名\]:\[id\]
2. 长度不超过44字节
3. 不包含特殊字符

优点： 可读性强、避免key冲突、方便管理、更节省内存(key是string类型，底层编码包含int、embstr和raw三种，embstr在小于44字节使用，采用连续内存空间，内存占用更小)

BigKey通常以key的大小和key中成员的数量来综合判定，例如：

    key本身数据量过大：一个String类型的key，它的值为5MB
    key中的成员数过多： 一个zset类型的key，它的成员数量为10000个
    key中成员的数据量过大： 一个hash类型的key，它的成员数量虽然只有1000个但是这些成员的Value值总大小为100MB

    推荐值： 
        1. 单个key的value小于10KB
        2. 对于集合类型的key，建议元素数量小于1000

BigKey的危害：

1. 网络阻塞

        对BigKey执行读请求时，少量的QPS就可能导致带宽使用率被占满，导致redis实例，乃至所在物理机都变慢
2. 数据倾斜

        BigKey所在的Redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡
3. Redis阻塞
        
        对元素较多的hash、list、zset等做运算会耗时较久，使主线程被阻塞
4. CPU压力

        对BigKey的数据序列化和反序列化会导致CPU的使用率飙升，影响Redis实例和本机其他应用

如何发现BigKey？

1. redis-cli --bigkeys

        利用redis-cli提供的 --bigkeys参数，可以遍历分析所有key，并返回Key的整体统计信息与每个数据的TOP1的big key
2. scan 扫描

        自己编程，利用scan扫描redis中的所有key，利用strlen、hlen等命令判断key的长度（不建议用MEMORY USAGE）
3. 第三方工具

        利用第三方工具，如Redis-Rdb-Tools分析RDB快照文件，全面分析内存使用情况
4. 网络监控

        自定义工具，监控进出Redis的网络数据，超出预警值时主动告警


如何删除BigKey

bigkey内存占用较多，即便是删除这样的key也需要耗费很长时间，导致Redis主线程阻塞，引发一系列问题

1. Redis 3.0及以下版本

        如果是集合类型，则遍历BigKey的元素，先逐个删除子元素，最后删除BigKey
2. Redis 4.0 以后

        Redis在4.0 以后提供了异步删除的命令，unlink

---

## 恰当的数据类型

当存储一个用户对象，我们有三种存储方式：

| 存储方式    | 数据格式                                                                                                                                                | 优缺点                                                 |
|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------|
| json字符串 | user:1 -> {"name":"jack","age":21}                                                                                                                  | 优点： 实现简单粗暴 <br> 缺点：数据耦合，不够灵活                        |
| 字段打散    | user:1:name -> "jack" <br> user:1:age -> 21                                                                                                         | 优点： 可以灵活访问对象任意字段 <br> 缺点： 占用空间大，没办法统一控制             |
| hash    | user:1 -> name \| age <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; jack &nbsp;&nbsp;\|&nbsp; 21  | 优点： 底层使用ziplist，占用空间小，可以灵活访问对象任意字段 <br> 缺点： 代码相对复杂  |


**注意：hash的entry个数超过500时，就使用hash表存储而不是ziplist，占用内存较多**

    可以通过调整 hash-max-ziplist-entries 来配置ziplist支持的entry上限。但是如果entry过多就会导致bigkey问题

例：

有一个hash类型的key 其中有100万对field和value，field是自增id，如何优化

    可以通过打散hash的方式，将100万个数据划分到10000个小的hash结构中，例如对field进行/100操作；然后对于每个小hash中的filed进行对
    100取模，即可均匀分散一百个数据，当需要取第98765个数据的时候：
        首先，确定所在hash  98765/100 = 987 所以是在 key:987 这个小hash里
        其次确定小hash中的元素位数： 98765%100 = 65， 所以最终数据就是在 key:987这个hash的field=65的entry中

---

## 批处理优化

单个命令执行时间 = 2*n次网络传输时间+1次命令执行时间

批处理命令执行时间=2次网络传输时间+n次命令执行时间

jedis.pipelined() 或者原生mxxx操作

    注意： pipeline不是原子性的 但是原生mxxx操作是原子性的

**不要在一次批处理中传输太多命令，否则单词命令占用带宽太多，会导致网络阻塞**

Mxx命令或pipeline这样的批处理需要在一次请求中携带多条命令，而此时如果Redis是一个集群，那批处理命令的多个key必须落在一个插槽中，否则会导致执行失败

解决方案：

| 方案   | 串行命令             | 串行slot                                                    | 并行slot                                             | hash_tag                               |
|------|------------------|-----------------------------------------------------------|----------------------------------------------------|----------------------------------------|
| 实现思路 | for循环遍历，依次执行每个命令 | 在客户端计算每个key的slot，将slot一致的分为一组，然后每组再用pipline机型批处理，串行执行各组命令 | 在客户端计算每个key的slot，将slot相同的分为一组，然后使用pipeline并行执行各组命令 | 将所有的key都设置相同的hash_tag,则所有的key的slot一定相同 |
| 耗时   | N次网络+N次命令耗时      | m次网络+n次命令<br>m=key的dlot个数                                 | 1次网络耗时+N次命令耗时                                      | 1次网络耗时+N次命令耗时                          |
| 优点   | 实现简单             | 耗时较短                                                      | 耗时非常短                                              | 耗时非常短、实现简单                             |
| 缺点   | 耗时非常久            | 实现稍复杂，slot越多耗时越久                                          | 实现复杂                                               | 容易出现数据倾斜                               |

---

## 持久化配置

持久化应遵循以下建议：

1. 用来做缓存的Redis实例尽量不要开启持久化功能
2. 建议关闭Rdb持久化功能，使用AOF持久化
3. 利用脚本定期在slave节点做RDB，实现数据备份
4. 设置合理的rewrite值，避免频繁的bgrewrite
5. 配置no-appendfsync-on-rewrite-yes 禁止在rewrite期间做aof，避免因aof引起的阻塞

部署有关建议：

1. Redis实例的物理机要预留足够内存，应对fork和rewrite
2. 单个Redis实例内存上限不要太大，例如4g或8g。可以加快fork速度，减少主从同步、数据迁移压力
3. 不要与cpu密集型应用部署在一起
4. 不要与高硬盘负载应用一起部署。例如：数据库，消息队列

---

## 慢查询

慢查询： 在Redis执行耗时超过某个阈值的命令，称为慢查询。

慢查询的阈值可以通过配置指定： slowlog-log-slower-than： 慢查询阈值，单位默认是微秒，默认是10000 建议 1000

慢查询会被放入慢查询日志中，日志的长度有上限，可以通过配置指定： slowlog-max-len： 慢查询日志的长度，默认是128， 建议1000 

查询慢查询日志的几个命令：

    slowlog len： 查询慢查询日志长度
    slowlog get [n]：读取n条慢查询日志
    slowlog reset： 清空慢查询日志列表

---

## 命令安全

一些建议：

1. Redis 一定要设置密码
2. 禁止线上使用下列命令： keys、flushall、flushdb、config set 等命令，可以利用rename-command禁用。
3. bing：限制网卡，禁止外网网卡访问
4. 开启防火墙
5. 不要使用Root账户启动redis
6. 尽量不使用默认的端口

---

## 内存配置

当Redis内存不足时，可能导致key被频繁删除、响应时间变长、QPS不稳定等问题。当内存使用率达到90%以上时就需要我们警惕，并快速定位到内存占用的原因

Redis 的内存划分

| 内存占用   | 说明                                                                                |
|--------|-----------------------------------------------------------------------------------|
| 数据内存   | 是Redis的最主要部分，存储Redis的键值信息。主要问题是BigKey问题、内存碎片问题                                    |
| 进程内存   | Redis主进程本身运行肯定需要占用内存，如代码、常量池等等；这部分内存大约几兆，在大多数生产环境中与Redis数据占用的内存相比可以忽略             |
| 缓冲区内存  | 一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端发缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，不当使用BigKey，可能导致内存益处  |

Redis提供了命令来查看内存分配状态  ： info memory  和 memory xxx

内存缓冲区配置


| 缓冲区名称   | 作用                                                                     |
|---------|------------------------------------------------------------------------|
| 复制缓冲区   | 主从复制的repl_backlog_buf，如果太小可能导致频繁的全量复制，影响性能。通过repl-backlog-size 设置默认1mb |
| AOF缓冲区  | AOF刷盘之前的缓冲区域，AOF执行rewrite的缓冲区。无法设置容量上限                                 |
| 客户端缓冲区  | 分为输入缓冲区和输出缓冲区，输入缓冲区最大1G且不能设置。输出缓冲区可以设置                                 |

---

## 集群最佳实践

1. 集群完整性问题
    
        在Redis的默认配置中，如果发现任意一个插槽不可用，则整个集群都会停止对外服务：为了保证高可用性，这里建议将cluster-require-full-coverage配置为false
2. 集群中节点越多，集群状态信息数据量也越大，此时每次集群互通需要的带宽会非常高。

        解决途径：
            1.避免大集群，集群节点数不要太多，最好少于1000，如果业务庞大，则建立多个集群。
            2.避免在单个物理机中运行太多Redis实例
            3.配置合适的cluster-node-timeout值
3. 数据倾斜问题
4. 客户端性能问题
5. 命令的集群兼容性问题
6. lua和事务的问题



